{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    tf.reset_default_graph()\n",
    "    ### load meta parameters, model and restore\n",
    "    settings = json.load(open(os.path.join(os.path.dirname(model_path), 'kwargs.json'), 'r'))\n",
    "    model = RotReacherLSTM(**settings)\n",
    "    model.saver.restore(model.session, model_path)\n",
    "    return model, settings\n",
    "\n",
    "def load_latest_model(model_dir):\n",
    "    filenames = glob.glob(os.path.join(model_dir, 'model_*.ckpt.index'))\n",
    "    ### find all model checkpoint in the subfolder and order them by their epoch\n",
    "    model_idx = sorted([int(re.findall(\n",
    "        r'(?<=model_)\\d+(?=.ckpt.index)', f)[0]) for f in filenames])\n",
    "    epoch = model_idx[-1]\n",
    "    model_dir = os.path.join(model_dir, 'model_{:d}.ckpt'.format(epoch))\n",
    "    print('Loading {}...'.format(model_dir))\n",
    "    model, settings = load_model(model_dir)\n",
    "    return model, settings, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotReacherLSTM(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.tfgraph = tf.Graph()\n",
    "        with self.tfgraph.as_default():\n",
    "            \n",
    "            ### input layer\n",
    "            self.state = tf.placeholder(tf.float32, shape=(None, None, 4), name='StateData')\n",
    "            self.act = tf.placeholder(tf.float32, shape=(None, None, 2), name='ActData')\n",
    "            self.is_reset = tf.placeholder(tf.float32, shape=(None, None, 1), name='ResetData')\n",
    "            self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "            input_scaling = 100.\n",
    "            vel_scaling = 100.\n",
    "            scaled_state = tf.concat([self.state[:,:,:2] * input_scaling, self.state[:,:,2:] * vel_scaling], axis=2)\n",
    "            scaled_act = self.act * input_scaling\n",
    "            scaled_is_reset = self.is_reset * 1.\n",
    "            \n",
    "            batch_size = tf.shape(self.state)[0]\n",
    "            \n",
    "            ### we don't put the last observation, action and reset bit into the lstm\n",
    "            lstm_inputs=tf.concat([scaled_state, scaled_act, scaled_is_reset], axis=2)[:,:-1,:]\n",
    "\n",
    "            ### LSTM layer \n",
    "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=kwargs['lstm_units'], forget_bias=1.0)\n",
    "            self.initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "            lstm_out, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, \n",
    "                                                           inputs=lstm_inputs,\n",
    "                                                           initial_state=self.initial_state,\n",
    "                                                           dtype=tf.float32)\n",
    "            fc1_inp = tf.reshape(lstm_out, (-1, kwargs['lstm_units']))\n",
    "            \n",
    "            ## two dense layers\n",
    "            fc1 = tf.layers.dense(fc1_inp, 100, activation=tf.nn.relu, name='fc1')\n",
    "            fc2 = tf.layers.dense(fc1, 100, activation=tf.nn.relu, name='fc2')\n",
    "            \n",
    "            ### outputs (observations and velocities in x and y), loss (MSE) and optimizer (Adam) definition\n",
    "            out = tf.layers.dense(fc2, 4, activation=None, name='out')\n",
    "\n",
    "            timesteps = tf.shape(lstm_out)[1]\n",
    "            self.scaled_pred_state = tf.reshape(out, (batch_size, timesteps, 4))\n",
    "            self.pred_state = tf.concat([self.scaled_pred_state[:,:,:2] / input_scaling, self.scaled_pred_state[:,:,2:] / vel_scaling], axis=2)\n",
    "            \n",
    "            ### the loss is the average error per time step in our current batch\n",
    "            #self.loss = tf.reduce_mean((self.scaled_pred_state-scaled_state[:,1:])**2)\n",
    "            self.loss = tf.reduce_mean(tf.norm(self.scaled_pred_state[:,:]-scaled_state[:,1:], axis=2))\n",
    "            self.avg_error = tf.reduce_mean(tf.norm(self.pred_state[:,:,:2]-self.state[:,1:,:2], axis=2))\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
    "\n",
    "            self.saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "            ### configure session (run on CPU) and initialize\n",
    "            session_config = tf.ConfigProto(device_count={'GPU': 0},\n",
    "                                        log_device_placement=False)\n",
    "            self.session = tf.Session(config=session_config)\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "                    \n",
    "\n",
    "    def get_loss_error(self, obs, vels, act, is_reset):\n",
    "        with self.tfgraph.as_default():\n",
    "            return self.session.run((self.loss, self.avg_error), \n",
    "                                    {self.state:np.concatenate([obs, vels], axis=2),\n",
    "                                     self.act:act, \n",
    "                                     self.is_reset:is_reset})\n",
    "    \n",
    "    def train(self, obs, vels, act, is_reset, learning_rate):\n",
    "        with self.tfgraph.as_default():\n",
    "            return self.session.run((self.loss, self.train_op),\n",
    "                                    {self.state:np.concatenate([obs, vels], axis=2),\n",
    "                                     self.act:act, \n",
    "                                     self.is_reset:is_reset,\n",
    "                                     self.learning_rate:learning_rate})\n",
    "\n",
    "    def predict_next_state(self, obs, vels, act, is_reset):\n",
    "        obs = np.array(obs)\n",
    "        vels = np.array(vels)\n",
    "        act = np.array(act)\n",
    "        is_reset = np.array(is_reset)\n",
    "        obs_help = np.zeros((obs.shape[0], obs.shape[1]+1, 2))\n",
    "        vels_help = np.zeros((vels.shape[0], vels.shape[1]+1, 2))\n",
    "        act_help = np.zeros((obs.shape[0], obs.shape[1]+1, 2))\n",
    "        is_reset_help = np.zeros((obs.shape[0], obs.shape[1]+1, 1))\n",
    "        obs_help[:,:obs.shape[1],:] = obs\n",
    "        vels_help[:,:vels.shape[1],:] = vels\n",
    "        act_help[:,:obs.shape[1],:] = act\n",
    "        is_reset_help[:,:obs.shape[1],:] = is_reset\n",
    "        \n",
    "        with self.tfgraph.as_default():\n",
    "            new_state = self.session.run((self.pred_state),\n",
    "                                {self.state:np.concatenate([obs_help, vels_help], axis=2), \n",
    "                                 self.act:act_help, \n",
    "                                 self.is_reset:is_reset_help})\n",
    "        ### return position and velocitiy\n",
    "        return new_state[:,:,:2], new_state[:,:,2:]\n",
    "        \n",
    "    def predict_trajectory(self, obs, vels, act, is_reset):\n",
    "        # Takes the observations and the first |obs| actions to compute the initial lstm state and predicts\n",
    "        # as many future states as there are additional actions plus one, so for example:\n",
    "        # s:        1,2,3,4\n",
    "        # a:        1,2,3,4,5,6,7,8\n",
    "        # is_reset: 0,0,0,0,0,0,0,0\n",
    "        # --> the function will return obs_predicted = 1,2,3,4,5,6,7,8,9\n",
    "        obs = np.array(obs)\n",
    "        vels = np.array(vels)\n",
    "        act = np.array(act)\n",
    "        is_reset = np.array(is_reset)\n",
    "        \n",
    "        ### compute initial state, initial to predicted trajectory\n",
    "        lstm_state = self.session.run(self.final_state, feed_dict=\n",
    "                                      {self.state:np.concatenate([obs, vels], axis=2), \n",
    "                                       self.act:act[:,:obs.shape[1],:], \n",
    "                                       self.is_reset:is_reset[:,:obs.shape[1],:]})\n",
    "        \n",
    "        all_obs = np.zeros((obs.shape[0], act.shape[1]+1, 2))\n",
    "        all_vels = np.zeros((obs.shape[0], act.shape[1]+1, 2))\n",
    "        all_act = np.zeros((obs.shape[0], act.shape[1]+1, 2))\n",
    "        all_is_reset = np.zeros((obs.shape[0], act.shape[1]+1, 1))\n",
    "        all_obs[:,:obs.shape[1],:] = obs\n",
    "        all_vels[:,:vels.shape[1],:] = vels\n",
    "        all_act[:,:act.shape[1],:] = act\n",
    "        all_is_reset[:,:act.shape[1],:] = is_reset\n",
    "        \n",
    "        all_states = np.concatenate([all_obs, all_vels], axis=2)\n",
    "        ### loop through all remaining actions and predict the next state (open loop)\n",
    "        for i in range(obs.shape[1]-1, act.shape[1]):\n",
    "            with self.tfgraph.as_default():\n",
    "                pred_states, lstm_state = self.session.run((self.pred_state, self.final_state), feed_dict={\n",
    "                                                    self.state: all_states[:,i:i+2,:],\n",
    "                                                    self.act: all_act[:,i:i+2,:],\n",
    "                                                    self.is_reset: all_is_reset[:,i:i+2,:],\n",
    "                                                    self.initial_state: lstm_state})\n",
    "            all_states[:,i+1] = pred_states.squeeze()\n",
    "\n",
    "        return all_states[:,:,:2], all_states[:,:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mode, train_data_path=None, lstm_units=100, unit='m', resume_training=False, **kwargs):\n",
    "    assert unit in ['m', 'cm']\n",
    "    \n",
    "    batch_size = kwargs['batch_size']\n",
    "    n_epochs = kwargs['n_epochs']\n",
    "    lr_schedule = np.array(kwargs['lr_schedule'])\n",
    "    kwargs['lstm_units'] = lstm_units\n",
    "    kwargs['mode'] = mode\n",
    "    \n",
    "    ### if no path is given, choose the latest training data and extract them\n",
    "    if train_data_path is None:\n",
    "        # choose latest directory\n",
    "        train_data_path = sorted([name for name in os.listdir(Path().resolve()) if os.path.isdir(name) and \n",
    "                                                                           'training_data' in name])[-1]\n",
    "    try:\n",
    "        train_dataset = pickle.load(open(os.path.join(train_data_path, 'train_data_{}.pkl'.format(mode)), 'rb'))\n",
    "        val_dataset = pickle.load(open(os.path.join(train_data_path, 'val_data_{}.pkl'.format(mode)), 'rb'))\n",
    "    except:\n",
    "        raise FileNotFoundError('Data not in directory')\n",
    "    print('Successfully opened training and validation data file {}/x_data_{}.'.format(train_data_path, mode))\n",
    "    \n",
    "    train_obs = train_dataset['obs']\n",
    "    train_vels = train_dataset['vels']\n",
    "    train_act = train_dataset['actions']\n",
    "    train_reset_bits = train_dataset['reset_bits']\n",
    "    val_obs = val_dataset['obs']\n",
    "    val_vels = val_dataset['vels']\n",
    "    val_act = val_dataset['actions']\n",
    "    val_reset_bits = val_dataset['reset_bits']\n",
    "    \n",
    "    unit_scaling = 100. if unit == 'm' else 1.\n",
    "\n",
    "    n_trajectories = train_obs.shape[0]\n",
    "    n_batches = n_trajectories // batch_size\n",
    "\n",
    "    batch_idx = np.linspace(0, n_trajectories, n_batches+1, dtype=int)\n",
    "    \n",
    "    ### initialize logged variables\n",
    "    train_loss = np.zeros(n_epochs) * np.nan\n",
    "    val_loss = np.zeros(n_epochs) * np.nan\n",
    "    train_error = np.zeros(n_epochs) * np.nan\n",
    "    val_error = np.zeros(n_epochs) * np.nan\n",
    "\n",
    "    ### load or initialize the model, depending on if we resume training or start from scratch\n",
    "    if not resume_training:\n",
    "        ### set up log foder\n",
    "        log_path = time.strftime('log/%d_%H-%M-%S_{}_{}/'.format(mode,lstm_units))\n",
    "        if not os.path.exists(log_path):\n",
    "            os.makedirs(log_path)\n",
    "        json.dump(kwargs, open(os.path.join(log_path, 'kwargs.json'), 'w'))\n",
    "        \n",
    "        ### initialize epoch\n",
    "        epoch = 1\n",
    "        \n",
    "        ### initialize model\n",
    "        model = RotReacherLSTM(**kwargs)\n",
    "    else:\n",
    "        ### extract directory names and find a match\n",
    "        dir_names = os.listdir('log')\n",
    "        dir_names_pure = list(map(lambda name: name.split('_')[-2]+'_'+name.split('_')[-1], dir_names))\n",
    "        match_idx = dir_names_pure.index(mode+'_'+str(lstm_units))\n",
    "        log_path = os.path.join('log', dir_names[match_idx])\n",
    "        print('Resume training in directory {}.'.format(log_path))\n",
    "        \n",
    "        ### load the latest model according to mode and #lstm units\n",
    "        model, _, epoch = load_latest_model(log_path)\n",
    "        \n",
    "        stats = pickle.load(open(os.path.join(log_path, 'stats.pkl'), 'rb'))\n",
    "        train_loss_hist = stats['train_loss']\n",
    "        val_loss_hist = stats['val_loss']\n",
    "        train_avg_error_hist = stats['train_error']\n",
    "        val_avg_error_hist = stats['val_error']\n",
    "        \n",
    "        train_loss[:epoch] = train_loss_hist[~np.isnan(train_loss_hist)]\n",
    "        val_loss[:epoch] = val_loss_hist[~np.isnan(val_loss_hist)]\n",
    "        train_error[:epoch] = train_avg_error_hist[~np.isnan(train_avg_error_hist)]\n",
    "        val_error[:epoch] = val_avg_error_hist[~np.isnan(val_avg_error_hist)]\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    ### time the training process\n",
    "    tf.reset_default_graph()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ### loop through epochs, train on minibatches and store the model every now and then\n",
    "    while epoch <= n_epochs:\n",
    "        t = time.time()\n",
    "        \n",
    "        ### draw learning rate according to schedule\n",
    "        learning_rate = lr_schedule[epoch>lr_schedule[:,0]][-1,1]\n",
    "        if epoch-1 in lr_schedule[:,0]:\n",
    "            print('New learning rate: {}'.format(learning_rate))\n",
    "        \n",
    "        ### shuffle the training data\n",
    "        perm = np.random.permutation(np.arange(n_trajectories))\n",
    "        train_obs = train_obs[perm]\n",
    "        train_vels = train_vels[perm]\n",
    "        train_act = train_act[perm]\n",
    "        train_reset_bits = train_reset_bits[perm]\n",
    "\n",
    "        ### loop through mini batches and train\n",
    "        for b in range(n_batches):\n",
    "            obs_batch = train_obs[batch_idx[b]:batch_idx[b+1]]\n",
    "            vels_batch = train_vels[batch_idx[b]:batch_idx[b+1]]\n",
    "            act_batch = train_act[batch_idx[b]:batch_idx[b+1]]\n",
    "            resets_batch = train_reset_bits[batch_idx[b]:batch_idx[b+1]]\n",
    "            loss, _ = model.train(obs_batch, vels_batch, act_batch, resets_batch, learning_rate)\n",
    "        \n",
    "        ### we are interested in the average error on the entire training/validation set\n",
    "        train_loss[epoch-1], avg_err = model.get_loss_error(train_obs, train_vels, train_act, train_reset_bits)\n",
    "        train_error[epoch-1] = avg_err * unit_scaling\n",
    "        val_loss[epoch-1], avg_err = model.get_loss_error(val_obs, val_vels, val_act, val_reset_bits)\n",
    "        val_error[epoch-1] = avg_err * unit_scaling\n",
    "\n",
    "        print('Epoch {:d} ({:.1f}s): train loss {:.5f} | val loss {:.5f} | train error {:.5f} cm | val error {:.5f} cm'.format(\n",
    "           epoch, time.time()-t, train_loss[epoch-1], val_loss[epoch-1], train_error[epoch-1], val_error[epoch-1]))\n",
    "\n",
    "        ### Save the model and statistics every n epochs\n",
    "        if epoch % 10 == 0 or epoch == n_epochs-1:\n",
    "            fn = model.saver.save(model.session,\n",
    "                   os.path.join(log_path, 'model_{:d}.ckpt'.format(epoch)))\n",
    "            stats = dict(train_loss=train_loss, val_loss=val_loss, train_error=train_error, val_error=val_error)\n",
    "            pickle.dump(stats, open(os.path.join(log_path, 'stats.pkl'), 'wb'))\n",
    "            print('Model saved:', fn)\n",
    "\n",
    "        epoch += 1\n",
    "    \n",
    "    print('Training took {} minutes.'.format((time.time()-start_time)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened training and validation data file training_data_14_22_12_29/x_data_rotplus.\n",
      "Resume training in directory log/14_22-23-10_rotplus_200.\n",
      "Loading log/14_22-23-10_rotplus_200/model_2520.ckpt...\n",
      "INFO:tensorflow:Restoring parameters from log/14_22-23-10_rotplus_200/model_2520.ckpt\n",
      "Epoch 2521 (7.2s): train loss 0.04154 | val loss 0.04923 | train error 0.03632 cm | val error 0.04332 cm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-fd26968a4fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-3f050a88b1bb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(mode, train_data_path, lstm_units, unit, resume_training, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mact_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_act\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mresets_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_reset_bits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvels_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresets_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m### we are interested in the average error on the entire training/validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Experiment_Scripts/train_lstm.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, obs, vels, act, is_reset, learning_rate)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow16/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow16/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow16/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow16/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow16/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### this section is for notebook execution\n",
    "settings = {'batch_size': 16,\n",
    "            'n_epochs': 4000,\n",
    "            #'lr_schedule': [[0, 1e-3], [30, 1e-4], [100, 5e-5], [200, 1e-5]]}\n",
    "            'lr_schedule': [[0, 1e-3], [30, 1e-4]]}\n",
    "\n",
    "### these are the parameters we want to iterate over\n",
    "units_arr = [100, 50, 150, 200]\n",
    "mode_arr = ['original','rot','rotplus']\n",
    "\n",
    "job_list = list(itertools.product(units_arr, mode_arr))\n",
    "\n",
    "\n",
    "# 1, 2 and 3 denote 'original', 'rot' and 'rot_plus' respectively\n",
    "# for the \"standard\" setting of 100 LSTM units\n",
    "job_id = 12\n",
    "\n",
    "job = job_list[job_id-1]\n",
    "train(lstm_units=job[0], mode=job[1], resume_training=True, **settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] job_id\n",
      "ipykernel_launcher.py: error: argument job_id: invalid int value: '/run/user/1000/jupyter/kernel-73e1a83a-a325-49fa-9b9e-28da7c689020.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hakany/tensorflow16/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "##### this section is for .py file #####\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('job_id', type=int)\n",
    "    args = vars(parser.parse_args())\n",
    "    job_id = args['job_id']\n",
    "\n",
    "    settings = {'batch_size': 16,\n",
    "            'n_epochs': 4000,\n",
    "            #'lr_schedule': [[0, 1e-3], [30, 1e-4], [100, 5e-5], [200, 1e-5]]}\n",
    "            'lr_schedule': [[0, 1e-3], [30, 1e-4]]}\n",
    "\n",
    "    ### these are the parameters we want to iterate over\n",
    "    units_arr = [100, 50, 150, 200]\n",
    "    mode_arr = ['original','rot','rotplus']\n",
    "\n",
    "    job_list = list(itertools.product(units_arr, mode_arr))\n",
    "\n",
    "\n",
    "    # job_id 1, 2 and 3 denote 'original', 'rot' and 'rot_plus' respectively\n",
    "    # for the \"standard\" setting of 100 LSTM units\n",
    "    job = job_list[job_id-1]\n",
    "    train(lstm_units=job[0], mode=job[1], resume_training=True, **settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
